{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a6e47a",
   "metadata": {},
   "source": [
    "# Simple Embedding  Models with a classificaiton head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14b44f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "upstream = None\n",
    "product = None\n",
    "\n",
    "model_name = \"mxbai-embed-large\"\n",
    "embedding_model = \"mxbai-embed-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32405609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46beca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directory exists:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    ROOT_DIR = \"../../data/processed/task4/subtask_4a/\"\n",
    "else:\n",
    "    ROOT_DIR = \"./data/processed/task4/subtask_4a/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cefcf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_with_labels(\n",
    "    texts: list[str], labels: list[float], embedding_model: str = \"mxbai-embed-large\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Generate embeddings for a given list of texts and their corresponding labels using the specified model.\n",
    "\n",
    "    Args:\n",
    "        texts (list[str]): A list of text strings to generate embeddings for.\n",
    "        labels (list[float]): A list of labels corresponding to the texts.\n",
    "        embedding_model (str): Name of the embedding model to use. Defaults to \"mxbai-embed-large\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing embeddings and their corresponding labels.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        result = ollama.embed(model=embedding_model, input=str(text))\n",
    "        embeddings.append({\"embedding\": result.embeddings[0], \"label\": label})\n",
    "    return pd.DataFrame(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce630ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"mxbai-embed-large\"\n",
    "columns = [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "# columns = [\"scientific_reference\", \"scientific_entities\"]\n",
    "\n",
    "\n",
    "# Load the provided trainning and test data:\n",
    "subtask4a_train_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"ct_train_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subtask4a_test_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"ct_dev_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "for cl in columns:\n",
    "    # Load the data\n",
    "    # Check if oversampling file exists:\n",
    "    if os.path.exists(os.path.join(ROOT_DIR, f\"ct_train_oversamples_{cl}.tsv\")):\n",
    "        subtask4a_cat_claim_train_df = pd.read_csv(\n",
    "            os.path.join(ROOT_DIR, f\"ct_train_oversamples_{cl}.tsv\"),\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"text\", cl],\n",
    "        )\n",
    "\n",
    "        print(f\"Evaluating {cl}...\")\n",
    "        # Generate embeddings for oversampling, training, and evaluation:\n",
    "        for dataset, name in [\n",
    "            (subtask4a_cat_claim_train_df, \"oversampling\"),\n",
    "            (subtask4a_train_df[[\"text\", cl]], \"training\"),\n",
    "            (subtask4a_test_df[[\"text\", cl]], \"evaluation\"),\n",
    "        ]:\n",
    "            embeddings_df = generate_embeddings_with_labels(\n",
    "                dataset[\"text\"].tolist(),\n",
    "                dataset[cl].tolist(),\n",
    "                embedding_model=embedding_model,\n",
    "            )\n",
    "            if name == \"oversampling\":\n",
    "                oversampling_embeddings_df = embeddings_df\n",
    "            elif name == \"training\":\n",
    "                standard_embeddings_df = embeddings_df\n",
    "            elif name == \"evaluation\":\n",
    "                eval_embeddings_df = embeddings_df\n",
    "\n",
    "        # Find the best model without using oversampling:\n",
    "        clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=f1_score)\n",
    "\n",
    "        X_train = np.array(standard_embeddings_df[\"embedding\"].tolist())\n",
    "        y_train = standard_embeddings_df[\"label\"].tolist()\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "        X_test = np.array(eval_embeddings_df[\"embedding\"].tolist())\n",
    "        y_test = eval_embeddings_df[\"label\"].tolist()\n",
    "\n",
    "        models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "        display(models.sort_values(\"f1_score\", ascending=False))\n",
    "\n",
    "        # Do it again with oversampling + training:\n",
    "        clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=f1_score)\n",
    "        X_train = np.array(pd.concat([standard_embeddings_df, oversampling_embeddings_df])[\"embedding\"].tolist())\n",
    "        y_train = pd.concat([standard_embeddings_df, oversampling_embeddings_df])[\"label\"].tolist()\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "        models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "        display(models.sort_values(\"f1_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348dbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d25379c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8,\n",
       " 'precision': 0.7777777777777778,\n",
       " 'recall': 0.8235294117647058,\n",
       " 'accuracy': 0.8978102189781022}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "metrics = {\n",
    "    \"f1\": f1_score(y_test, preds),\n",
    "    \"precision\": precision_score(y_test, preds),\n",
    "    \"recall\": recall_score(y_test, preds),\n",
    "    \"accuracy\": accuracy_score(y_test, preds),\n",
    "}\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat2025-task4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
