{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37dd6fea",
   "metadata": {},
   "source": [
    "# Hierachy Version of the Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b96a6e",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "upstream = None\n",
    "product = None\n",
    "some_param = None\n",
    "\n",
    "model_id = \"cardiffnlp/twitter-roberta-base-2022-154m\"\n",
    "warmup_ratio = 0.1\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 10\n",
    "weight_decay = 0.01\n",
    "per_device_train_batch_size = 32\n",
    "per_device_eval_batch_size = 256\n",
    "fp16 = False\n",
    "fp16_full_eval = False\n",
    "gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c05db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from climatesense_checkthat2025.utils.data import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9170519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eaa4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics_sequenceclassification(eval_predictions: EvalPrediction):\n",
    "    \"\"\"Compute metrics for a sequence classification model's predictions.\n",
    "\n",
    "    This function applies a softmax activation function to the model's raw logits to calculate probabilities,\n",
    "    converts probabilities to binary predictions based on a specified threshold, and computes evaluation metrics\n",
    "    using the provided `compute_metrics` function.\n",
    "\n",
    "    Args:\n",
    "        eval_predictions (EvalPrediction): An object containing the model's predictions and the true labels.\n",
    "            - `eval_predictions.predictions`: The raw logits output by the model.\n",
    "            - `eval_predictions.label_ids`: The true labels for the predictions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed evaluation metrics.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_predictions\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels),\n",
    "        \"precision\": precision.compute(predictions=predictions, references=labels),\n",
    "        \"recall\": recall.compute(predictions=predictions, references=labels),\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics_multilabel_sequenceclassification(\n",
    "    eval_predictions: EvalPrediction, threshold: float = 0.5, labels: List[str] = None\n",
    "):\n",
    "    \"\"\"Compute metrics for a multi-label sequence classification model's predictions.\n",
    "\n",
    "    This function applies a sigmoid activation function to the model's raw logits to calculate probabilities,\n",
    "    converts probabilities to binary predictions based on a specified threshold, and computes evaluation metrics\n",
    "    using the provided `compute_metrics` function.\n",
    "\n",
    "    Args:\n",
    "        eval_predictions (EvalPrediction): An object containing the model's predictions and the true labels.\n",
    "            - `eval_predictions.predictions`: The raw logits output by the model.\n",
    "            - `eval_predictions.label_ids`: The true labels for the predictions.\n",
    "        threshold (float, optional): The threshold for converting probabilities to binary predictions. Defaults to 0.5.\n",
    "        labels (List[str], optional): A list of label names for the metrics computation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed evaluation metrics.\n",
    "    \"\"\"\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    x_test = eval_predictions.predictions\n",
    "    y_test = eval_predictions.label_ids\n",
    "\n",
    "    # Calculate probabilities and derive binary predictions:\n",
    "    probs = sigmoid(torch.Tensor(x_test))\n",
    "    y_pred = torch.where(probs >= threshold, 1.0, 0.0)\n",
    "\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu()\n",
    "\n",
    "    return compute_metrics(y_pred, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6df78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 1228\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 137\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataset:\n",
    "\n",
    "import os.path\n",
    "\n",
    "if os.path.isfile(\"./data/processed/task4/subtask_4a/ct_train_clean.tsv\"):\n",
    "    subtask4a_train_df = pd.read_csv(\"./data/processed/task4/subtask_4a/ct_train_clean.tsv\", sep=\"\\t\")\n",
    "    subtask4a_dev_df = pd.read_csv(\"./data/processed/task4/subtask_4a/ct_dev_clean.tsv\", sep=\"\\t\")\n",
    "else:\n",
    "    subtask4a_train_df = pd.read_csv(\"../../data/processed/task4/subtask_4a/ct_train_clean.tsv\", sep=\"\\t\")\n",
    "    subtask4a_dev_df = pd.read_csv(\"../../data/processed/task4/subtask_4a/ct_dev_clean.tsv\", sep=\"\\t\")\n",
    "\n",
    "ds_train = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": subtask4a_train_df[\"text\"],\n",
    "        \"labels\": subtask4a_train_df[\n",
    "            [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "        ].values.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_dev = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": subtask4a_dev_df[\"text\"],\n",
    "        \"labels\": subtask4a_dev_df[[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]].values.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds = DatasetDict({\"train\": ds_train, \"test\": ds_dev})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394db60",
   "metadata": {},
   "source": [
    "## Create Zero Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ones dataset:\n",
    "def check_all_zero(row):\n",
    "    return all(value == 0.0 for value in row[[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]])\n",
    "\n",
    "\n",
    "subtask4a_train_df[\"all_zero\"] = subtask4a_train_df.apply(check_all_zero, axis=1)\n",
    "subtask4a_dev_df[\"all_zero\"] = subtask4a_dev_df.apply(check_all_zero, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df4cf344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_entities</th>\n",
       "      <th>all_zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1046</td>\n",
       "      <td>@user those eyes are a gift send straight from...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>638</td>\n",
       "      <td>Remember when libs attacked @user for his conc...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1091</td>\n",
       "      <td>Teenage Fever is a mood</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>Steam survey shows PC gamers are still mostly ...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142</td>\n",
       "      <td>Key findings utilized by Watson and Crick: Fra...</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>296</td>\n",
       "      <td>Hey everyone, here's a great thing from @user ...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>94</td>\n",
       "      <td>The War On Science: What It Is And How To Win ...</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>1240</td>\n",
       "      <td>https://tonic.vice.com/en_us/article/8xz9mz/ma...</td>\n",
       "      <td>[1.0, 1.0, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>637</td>\n",
       "      <td>@user Martha - stop the redundant BS!! Even if...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>286</td>\n",
       "      <td>'@user: During Medieval times, red hair was as...</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1228 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text  \\\n",
       "0      1046  @user those eyes are a gift send straight from...   \n",
       "1       638  Remember when libs attacked @user for his conc...   \n",
       "2      1091                            Teenage Fever is a mood   \n",
       "3        31  Steam survey shows PC gamers are still mostly ...   \n",
       "4       142  Key findings utilized by Watson and Crick: Fra...   \n",
       "...     ...                                                ...   \n",
       "1223    296  Hey everyone, here's a great thing from @user ...   \n",
       "1224     94  The War On Science: What It Is And How To Win ...   \n",
       "1225   1240  https://tonic.vice.com/en_us/article/8xz9mz/ma...   \n",
       "1226    637  @user Martha - stop the redundant BS!! Even if...   \n",
       "1227    286  '@user: During Medieval times, red hair was as...   \n",
       "\n",
       "               labels  scientific_claim  scientific_reference  \\\n",
       "0     [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "1     [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "2     [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "3     [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "4     [0.0, 0.0, 1.0]               0.0                   0.0   \n",
       "...               ...               ...                   ...   \n",
       "1223  [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "1224  [0.0, 0.0, 1.0]               0.0                   0.0   \n",
       "1225  [1.0, 1.0, 1.0]               1.0                   1.0   \n",
       "1226  [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "1227  [1.0, 0.0, 0.0]               1.0                   0.0   \n",
       "\n",
       "      scientific_entities  all_zero  \n",
       "0                     0.0      True  \n",
       "1                     0.0      True  \n",
       "2                     0.0      True  \n",
       "3                     0.0      True  \n",
       "4                     1.0     False  \n",
       "...                   ...       ...  \n",
       "1223                  0.0      True  \n",
       "1224                  1.0     False  \n",
       "1225                  1.0     False  \n",
       "1226                  0.0      True  \n",
       "1227                  0.0     False  \n",
       "\n",
       "[1228 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtask4a_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e90d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1228\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 137\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset for the ones:\n",
    "\n",
    "ds_train_zeros = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": subtask4a_train_df[\"text\"],\n",
    "        \"label\": list(map(int, subtask4a_train_df[\"all_zero\"])),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_dev_zeros = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": subtask4a_dev_df[\"text\"],\n",
    "        \"label\": list(map(int, subtask4a_dev_df[\"all_zero\"])),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_zeros = DatasetDict({\"train\": ds_train_zeros, \"test\": ds_dev_zeros})\n",
    "ds_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_loss(outputs, labels, num_items_in_batch=None, return_outputs=False, class_weights=None):\n",
    "    logits = outputs.get(\"logits\")\n",
    "    n_labels = logits.shape[1]\n",
    "\n",
    "    if class_weights is not None and len(class_weights) == n_labels:\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).float().to(device=logits.device))\n",
    "    else:\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits.view(-1, n_labels), labels.view(-1))\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114f0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.26859504, 0.82526882])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(ds_zeros[\"train\"][\"label\"]),\n",
    "    y=ds_zeros[\"train\"][\"label\"],\n",
    ").astype(float)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c407ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = f\"baseline-{model_id.split('/')[-1]}\"\n",
    "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\"\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    "zero_model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2, trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer=tokenizer):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_ds_zeros = ds_zeros.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "zero_trainer = Trainer(\n",
    "    model=zero_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_zeros[\"train\"],\n",
    "    eval_dataset=tokenized_ds_zeros[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_sequenceclassification,\n",
    "    compute_loss_func=partial(compute_weighted_loss, class_weights=class_weights),\n",
    "    callbacks=callbacks,  # FIXME should use an eval dataset for early stopping rather than the test set. Check: https://github.com/huggingface/setfit/issues/424\n",
    ")\n",
    "\n",
    "zero_trainer.train()\n",
    "zero_eval_result = zero_trainer.evaluate()\n",
    "zero_eval_result[\"model\"] = model_id\n",
    "pprint(zero_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_preds = zero_trainer.predict(tokenized_ds_zeros[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50a467",
   "metadata": {},
   "source": [
    "## Create Standard Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6a8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 484\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 42\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create subset dataset for the second sub task:\n",
    "\n",
    "subtask4a_train_df_sub = subtask4a_train_df[subtask4a_train_df[\"all_zero\"] == False]  # noqa: E712\n",
    "subtask4a_dev_df_sub = subtask4a_dev_df[subtask4a_dev_df[\"all_zero\"] == False]  # noqa: E712\n",
    "\n",
    "ds_train_sub = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": subtask4a_train_df_sub[\"text\"],\n",
    "        \"labels\": subtask4a_train_df_sub[\n",
    "            [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "        ].values.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_dev_sub = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": subtask4a_dev_df_sub[\"text\"],\n",
    "        \"labels\": subtask4a_dev_df_sub[\n",
    "            [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "        ].values.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_sub = DatasetDict({\"train\": ds_train_sub, \"test\": ds_dev_sub})\n",
    "ds_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93056d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = f\"baseline-{model_id.split('/')[-1]}\"\n",
    "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\"\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    num_labels=3,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer=tokenizer):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_ds_sub = ds_sub.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    # fp16=fp16,\n",
    "    # fp16_full_eval=fp16_full_eval,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_sub[\"train\"],\n",
    "    eval_dataset=tokenized_ds_sub[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=partial(\n",
    "        compute_metrics_multilabel_sequenceclassification,\n",
    "        labels=[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate()\n",
    "eval_result[\"model\"] = model_id\n",
    "\n",
    "pprint(eval_result)\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(tokenized_ds[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce06393",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "for zeros, p in zip(zero_preds.predictions, preds.predictions):\n",
    "    if np.argmax(zeros) == 1:\n",
    "        final_preds.append([0, 0, 0])\n",
    "    else:\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        p = sigmoid(torch.Tensor(p))\n",
    "        p = torch.where(p >= 0.5, 1.0, 0.0)\n",
    "        p = p.cpu().numpy()\n",
    "        final_preds.append(p)\n",
    "\n",
    "compute_metrics(\n",
    "    np.array(final_preds),\n",
    "    ds[\"test\"][\"labels\"],\n",
    "    labels=[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatesense-checkthat2025-task4-F-tagyMC-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
