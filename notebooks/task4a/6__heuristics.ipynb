{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b8a4e2",
   "metadata": {},
   "source": [
    "# Classifier using heuristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b05f4f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "upstream = None\n",
    "product = None\n",
    "\n",
    "model_name = \"mxbai-embed-large\"\n",
    "embedding_model = \"mxbai-embed-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dad49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directory exists:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    ROOT_DIR = \"../../data/processed/task4/subtask_4a/\"\n",
    "else:\n",
    "    ROOT_DIR = \"./data/processed/task4/subtask_4a/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4645b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ture/false with 0/1:\n",
    "def replace_true_false_with_0_1(df):\n",
    "    df = df.replace({True: 1, False: 0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17423c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_columns = [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "features = [\n",
    "    \"is_claim_with_sciterm\",\n",
    "    \"is_claim\",\n",
    "    \"contains_arg\",\n",
    "    \"contains_scientific_term\",\n",
    "    \"has_url\",\n",
    "    \"has_sci_domain\",\n",
    "    \"has_sci_subdomain\",\n",
    "    \"has_sci_mag_domain\",\n",
    "    \"has_sci_news_domain\",\n",
    "    \"is_related_to_research\",\n",
    "    \"mentions_science_research_in_general\",\n",
    "    \"mentions_scientist\",\n",
    "    \"mentions_publications\",\n",
    "    \"mentions_research_method\",\n",
    "]\n",
    "\n",
    "# Load the provided trainning and test data:\n",
    "subtask4a_train_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"ct_train_clean_heuristics.csv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subtask4a_test_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"ct_dev_clean_heuristics.csv\"),\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc265c",
   "metadata": {},
   "source": [
    "##Â With embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b51c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts: list[str], embedding_model: str = \"mxbai-embed-large\") -> list[np.ndarray]:\n",
    "    \"\"\"Generate embeddings for a given list of texts and their corresponding labels using the specified model.\n",
    "\n",
    "    Args:\n",
    "        texts (list[str]): A list of text strings to generate embeddings for.\n",
    "        labels (list[float]): A list of labels corresponding to the texts.\n",
    "        embedding_model (str): Name of the embedding model to use. Defaults to \"mxbai-embed-large\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing embeddings and their corresponding labels.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        result = ollama.embed(model=embedding_model, input=str(text))\n",
    "        embeddings.append(result.embeddings[0])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do it again with the embeddings:\n",
    "\n",
    "for cl in labels_columns:\n",
    "    print(f\"Evaluating {cl}...\")\n",
    "    subtask4a_cat_claim_train_df = pd.read_csv(\n",
    "        os.path.join(ROOT_DIR, f\"ct_train_oversamples_{cl}_heuristics.tsv\"),\n",
    "        sep=\"\\t\",\n",
    "    )\n",
    "\n",
    "    # Generate embeddings for oversampling, training, and evaluation:\n",
    "    for dataset, name in [\n",
    "        (subtask4a_train_df[[\"text\", cl] + features], \"training\"),\n",
    "        (subtask4a_cat_claim_train_df[[\"text\", cl] + features], \"oversampling\"),\n",
    "        (subtask4a_test_df[[\"text\", cl] + features], \"evaluation\"),\n",
    "    ]:\n",
    "        if name == \"training\":\n",
    "            train_df = dataset\n",
    "            train_df[\"embeddings\"] = generate_embeddings(train_df[\"text\"].tolist(), embedding_model=embedding_model)\n",
    "        if name == \"oversampling\":\n",
    "            oversampling_df = dataset\n",
    "            oversampling_df[\"embeddings\"] = generate_embeddings(\n",
    "                oversampling_df[\"text\"].tolist(), embedding_model=embedding_model\n",
    "            )\n",
    "        elif name == \"evaluation\":\n",
    "            test_df = dataset\n",
    "            test_df[\"embeddings\"] = generate_embeddings(test_df[\"text\"].tolist(), embedding_model=embedding_model)\n",
    "\n",
    "    # Find the best model without using oversampling:\n",
    "    clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=f1_score)\n",
    "\n",
    "    X_train = np.array(\n",
    "        [\n",
    "            x + y\n",
    "            for x, y in zip(\n",
    "                train_df[\"embeddings\"].values.tolist(),\n",
    "                train_df[features].replace({True: 1, False: 0}).values.tolist(),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_train = train_df[cl].tolist()\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    X_test = np.array(\n",
    "        [\n",
    "            x + y\n",
    "            for x, y in zip(\n",
    "                test_df[\"embeddings\"].values.tolist(),\n",
    "                test_df[features].replace({True: 1, False: 0}).values.tolist(),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    y_test = test_df[cl].tolist()\n",
    "\n",
    "    models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "    display(models.sort_values(\"f1_score\", ascending=False))\n",
    "\n",
    "    # Do it again with oversampling + training:\n",
    "    clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=f1_score)\n",
    "    X_train = np.array(\n",
    "        [\n",
    "            x + y\n",
    "            for x, y in zip(\n",
    "                train_df[\"embeddings\"].values.tolist(),\n",
    "                train_df[features].replace({True: 1, False: 0}).values.tolist(),\n",
    "            )\n",
    "        ]\n",
    "        + [\n",
    "            x + y\n",
    "            for x, y in zip(\n",
    "                oversampling_df[\"embeddings\"].values.tolist(),\n",
    "                oversampling_df[features].replace({True: 1, False: 0}).values.tolist(),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_train = pd.concat([train_df, oversampling_df])[cl].tolist()\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "    display(models.sort_values(\"f1_score\", ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat2025-task4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
