{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532ba038",
   "metadata": {},
   "source": [
    "# Baselines but using separate predictors and the augmented data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c9bd3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# parameters\n",
    "upstream = None\n",
    "product = None\n",
    "some_param = None\n",
    "\n",
    "model_id = \"microsoft/deberta-large-mnli\"\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directory exists:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    ROOT_DIR = \"../../data/processed/task4/subtask_4a/\"\n",
    "else:\n",
    "    ROOT_DIR = \"./data/processed/task4/subtask_4a/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735a4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1974814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics_sequenceclassification(eval_predictions: EvalPrediction):\n",
    "    \"\"\"Compute metrics for a sequence classification model's predictions.\n",
    "\n",
    "    This function applies a softmax activation function to the model's raw logits to calculate probabilities,\n",
    "    converts probabilities to binary predictions based on a specified threshold, and computes evaluation metrics\n",
    "    using the provided `compute_metrics` function.\n",
    "\n",
    "    Args:\n",
    "        eval_predictions (EvalPrediction): An object containing the model's predictions and the true labels.\n",
    "            - `eval_predictions.predictions`: The raw logits output by the model.\n",
    "            - `eval_predictions.label_ids`: The true labels for the predictions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed evaluation metrics.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_predictions\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels),\n",
    "        \"precision\": precision.compute(predictions=predictions, references=labels),\n",
    "        \"recall\": recall.compute(predictions=predictions, references=labels),\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"mxbai-embed-large\"\n",
    "columns = [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "# columns = [\"scientific_reference\", \"scientific_entities\"]\n",
    "\n",
    "\n",
    "# Load the provided trainning and test data:\n",
    "subtask4a_train_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"ct_train_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subtask4a_test_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"ct_dev_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_loss(outputs, labels, num_items_in_batch=None, return_outputs=False, class_weights=None):\n",
    "    logits = outputs.get(\"logits\")\n",
    "    n_labels = logits.shape[1]\n",
    "\n",
    "    if class_weights is not None and len(class_weights) == n_labels:\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).float().to(device=logits.device))\n",
    "    else:\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logits.view(-1, n_labels), labels.view(-1))\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00185a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "def fine_tune_binary_classifier(\n",
    "    model_name: str,\n",
    "    train_texts: List[str],\n",
    "    train_labels: List[int],\n",
    "    val_texts: List[str],\n",
    "    val_labels: List[int],\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 16,\n",
    ") -> Trainer:\n",
    "    \"\"\"Fine-tune a binary classifier using Hugging Face Transformers.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Pretrained model name (e.g., 'bert-base-uncased').\n",
    "        train_texts (List[str]): List of training texts.\n",
    "        train_labels (List[int]): List of training labels (0 or 1).\n",
    "        val_texts (List[str]): List of validation texts.\n",
    "        val_labels (List[int]): List of validation labels (0 or 1).\n",
    "        output_dir (str): Directory to save the fine-tuned model.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: Hugging Face Trainer object after training.\n",
    "    \"\"\"\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, trust_remote_code=True)\n",
    "\n",
    "    def preprocess_function(examples, tokenizer=tokenizer):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Create Dataset:\n",
    "    ds_train = Dataset.from_dict({\"text\": train_texts, \"labels\": train_labels})\n",
    "    ds_dev = Dataset.from_dict({\"text\": val_texts, \"labels\": val_labels})\n",
    "    ds = DatasetDict({\"train\": ds_train, \"test\": ds_dev})\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_ds = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "    # Define data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=2e-5,  # learning_rate=1e-6,\n",
    "        weight_decay=0.01,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        # metric_for_best_model=\"f1\",\n",
    "        # greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    # Define Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds[\"train\"].with_format(\"torch\"),\n",
    "        eval_dataset=tokenized_ds[\"test\"].with_format(\"torch\"),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_sequenceclassification,\n",
    "        data_collator=data_collator,\n",
    "        # compute_loss_func=compute_weighted_loss,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f556edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in columns:\n",
    "    # Load the data\n",
    "    # Check if oversampling file exists:\n",
    "    if os.path.exists(os.path.join(ROOT_DIR, f\"ct_train_oversamples_{cl}.tsv\")):\n",
    "        subtask4a_cat_claim_train_df = pd.read_csv(\n",
    "            os.path.join(ROOT_DIR, f\"ct_train_oversamples_{cl}.tsv\"),\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"text\", cl],\n",
    "        )\n",
    "\n",
    "        print(f\"Evaluating {cl}...\")\n",
    "        for dataset_df, name in [\n",
    "            (subtask4a_cat_claim_train_df, \"oversampling\"),\n",
    "            (subtask4a_train_df[[\"text\", cl]], \"training\"),\n",
    "            (subtask4a_test_df[[\"text\", cl]], \"evaluation\"),\n",
    "        ]:\n",
    "            if name == \"oversampling\":\n",
    "                oversampling_train_df = dataset_df\n",
    "            elif name == \"training\":\n",
    "                train_df = dataset_df = dataset_df\n",
    "            elif name == \"evaluation\":\n",
    "                eval_df = dataset_df = dataset_df\n",
    "\n",
    "        # Train model with oversampling + training:\n",
    "        X_train = np.array(pd.concat([train_df, oversampling_train_df])[\"text\"].tolist())\n",
    "        y_train = list(map(int, pd.concat([train_df, oversampling_train_df])[cl]))\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "        X_test = np.array(subtask4a_test_df[\"text\"].tolist())\n",
    "        y_test = list(map(int, subtask4a_test_df[cl]))\n",
    "\n",
    "        trainer = fine_tune_binary_classifier(\n",
    "            model_name=model_id,\n",
    "            train_texts=X_train,\n",
    "            train_labels=y_train,\n",
    "            val_texts=X_test,\n",
    "            val_labels=y_test,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # Evaluate the model:\n",
    "        eval_result = trainer.evaluate()\n",
    "        eval_result[\"model\"] = model_id\n",
    "\n",
    "        pprint(eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatesense-checkthat2025-task4-F-tagyMC-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
