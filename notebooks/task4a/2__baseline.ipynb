{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Baselines\n",
    "\n",
    "Rewritten baseline example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from climatesense_checkthat2025_task4.utils.data import compute_metrics, create_multilabel_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics_multilabel_sequenceclassification(\n",
    "    eval_predictions: EvalPrediction, threshold: float = 0.5, labels: List[str] = None\n",
    "):\n",
    "    \"\"\"Compute metrics for a multi-label sequence classification model's predictions.\n",
    "\n",
    "    This function applies a sigmoid activation function to the model's raw logits to calculate probabilities,\n",
    "    converts probabilities to binary predictions based on a specified threshold, and computes evaluation metrics\n",
    "    using the provided `compute_metrics` function.\n",
    "\n",
    "    Args:\n",
    "        eval_predictions (EvalPrediction): An object containing the model's predictions and the true labels.\n",
    "            - `eval_predictions.predictions`: The raw logits output by the model.\n",
    "            - `eval_predictions.label_ids`: The true labels for the predictions.\n",
    "        threshold (float, optional): The threshold for converting probabilities to binary predictions. Defaults to 0.5.\n",
    "        labels (List[str], optional): A list of label names for the metrics computation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed evaluation metrics.\n",
    "    \"\"\"\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    x_test = eval_predictions.predictions\n",
    "    y_test = eval_predictions.label_ids\n",
    "\n",
    "    # Calculate probabilities and derive binary predictions:\n",
    "    probs = sigmoid(torch.Tensor(x_test))\n",
    "    y_pred = torch.where(probs >= threshold, 1.0, 0.0)\n",
    "\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu()\n",
    "\n",
    "    return compute_metrics(y_pred, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 1091\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 273\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 1091\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 273\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 1091\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 273\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 1091\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 273\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 1092\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'labels'],\n",
       "         num_rows: 272\n",
       "     })\n",
       " })]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataset:\n",
    "subtask4a_df = pd.read_csv(\"../../data/processed/task4/subtask_4a/ct_train_data_clean.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "ids = subtask4a_df.index.values\n",
    "texts = subtask4a_df.text.values\n",
    "labels = subtask4a_df[[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]].values.tolist()\n",
    "full_ds = Dataset.from_dict({\"text\": texts, \"labels\": labels})\n",
    "\n",
    "\n",
    "# Create folds:\n",
    "folds_ds = create_multilabel_folds(full_ds, n_splits=5, random_state=1435892670)\n",
    "folds_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "evals = []\n",
    "model_id = \"cardiffnlp/twitter-roberta-base-2022-154m\"\n",
    "for fold, ds in tqdm(enumerate(folds_ds), desc=\"Running folds\", total=len(folds_ds)):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_id,\n",
    "        num_labels=3,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "    )\n",
    "\n",
    "    tokenizer_config = {\"pretrained_model_name_or_path\": model_id}\n",
    "    if \"scibert\" in model_id:\n",
    "        tokenizer_config[\"do_lower_case\"] = False\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(**tokenizer_config)\n",
    "\n",
    "    def tokenize_function(examples, tokenizer=tokenizer):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    tokenized_train_dataset = Dataset.from_dict(ds[\"train\"][:]).map(tokenize_function, batched=True)\n",
    "    tokenized_test_dataset = Dataset.from_dict(ds[\"test\"][:]).map(tokenize_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=256,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_test_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=partial(\n",
    "            compute_metrics_multilabel_sequenceclassification,\n",
    "            labels=[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    eval_result[\"fold\"] = fold + 1\n",
    "    eval_result[\"model\"] = model_id\n",
    "    eval_result = {key.lstrip(\"eval_\"): value for key, value in eval_result.items()}\n",
    "\n",
    "    pprint(eval_result)\n",
    "\n",
    "    evals.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>oss</th>\n",
       "      <th>scientific_claim_avg_acc</th>\n",
       "      <th>scientific_claim_avg_prec</th>\n",
       "      <th>scientific_claim_avg_rec</th>\n",
       "      <th>scientific_claim_avg_f1</th>\n",
       "      <th>scientific_reference_avg_acc</th>\n",
       "      <th>scientific_reference_avg_prec</th>\n",
       "      <th>scientific_reference_avg_rec</th>\n",
       "      <th>scientific_reference_avg_f1</th>\n",
       "      <th>scientific_entities_avg_acc</th>\n",
       "      <th>...</th>\n",
       "      <th>scientific_entities_avg_rec</th>\n",
       "      <th>scientific_entities_avg_f1</th>\n",
       "      <th>macro_acc</th>\n",
       "      <th>macro_prec</th>\n",
       "      <th>macro_rec</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>runtime</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>poch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">cardiffnlp/twitter-roberta-base-2022-154m</th>\n",
       "      <th>1</th>\n",
       "      <td>0.373783</td>\n",
       "      <td>0.875458</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.774648</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.919414</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.825806</td>\n",
       "      <td>0.772894</td>\n",
       "      <td>0.738288</td>\n",
       "      <td>0.871941</td>\n",
       "      <td>0.797756</td>\n",
       "      <td>24.1422</td>\n",
       "      <td>11.308</td>\n",
       "      <td>0.083</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.256274</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.930403</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.910345</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.826342</td>\n",
       "      <td>0.858715</td>\n",
       "      <td>0.841327</td>\n",
       "      <td>21.4280</td>\n",
       "      <td>12.740</td>\n",
       "      <td>0.093</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.356733</td>\n",
       "      <td>0.893773</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.807947</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>0.754544</td>\n",
       "      <td>0.795993</td>\n",
       "      <td>0.774218</td>\n",
       "      <td>18.8865</td>\n",
       "      <td>14.455</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.374265</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.783883</td>\n",
       "      <td>0.765873</td>\n",
       "      <td>0.805904</td>\n",
       "      <td>0.784103</td>\n",
       "      <td>19.8471</td>\n",
       "      <td>13.755</td>\n",
       "      <td>0.101</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.288283</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.919118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.790441</td>\n",
       "      <td>0.783911</td>\n",
       "      <td>0.879673</td>\n",
       "      <td>0.827211</td>\n",
       "      <td>23.1913</td>\n",
       "      <td>11.729</td>\n",
       "      <td>0.086</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     oss  \\\n",
       "model                                     fold             \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1     0.373783   \n",
       "                                          2     0.256274   \n",
       "                                          3     0.356733   \n",
       "                                          4     0.374265   \n",
       "                                          5     0.288283   \n",
       "\n",
       "                                                scientific_claim_avg_acc  \\\n",
       "model                                     fold                             \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                     0.875458   \n",
       "                                          2                     0.897436   \n",
       "                                          3                     0.893773   \n",
       "                                          4                     0.897436   \n",
       "                                          5                     0.882353   \n",
       "\n",
       "                                                scientific_claim_avg_prec  \\\n",
       "model                                     fold                              \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                      0.753425   \n",
       "                                          2                      0.805556   \n",
       "                                          3                      0.762500   \n",
       "                                          4                      0.761905   \n",
       "                                          5                      0.717391   \n",
       "\n",
       "                                                scientific_claim_avg_rec  \\\n",
       "model                                     fold                             \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                     0.774648   \n",
       "                                          2                     0.805556   \n",
       "                                          3                     0.859155   \n",
       "                                          4                     0.888889   \n",
       "                                          5                     0.916667   \n",
       "\n",
       "                                                scientific_claim_avg_f1  \\\n",
       "model                                     fold                            \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                    0.763889   \n",
       "                                          2                    0.805556   \n",
       "                                          3                    0.807947   \n",
       "                                          4                    0.820513   \n",
       "                                          5                    0.804878   \n",
       "\n",
       "                                                scientific_reference_avg_acc  \\\n",
       "model                                     fold                                 \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                         0.919414   \n",
       "                                          2                         0.930403   \n",
       "                                          3                         0.890110   \n",
       "                                          4                         0.904762   \n",
       "                                          5                         0.937500   \n",
       "\n",
       "                                                scientific_reference_avg_prec  \\\n",
       "model                                     fold                                  \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                          0.725806   \n",
       "                                          2                          0.816327   \n",
       "                                          3                          0.692308   \n",
       "                                          4                          0.750000   \n",
       "                                          5                          0.823529   \n",
       "\n",
       "                                                scientific_reference_avg_rec  \\\n",
       "model                                     fold                                 \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                             0.90   \n",
       "                                          2                             0.80   \n",
       "                                          3                             0.72   \n",
       "                                          4                             0.72   \n",
       "                                          5                             0.84   \n",
       "\n",
       "                                                scientific_reference_avg_f1  \\\n",
       "model                                     fold                                \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                        0.803571   \n",
       "                                          2                        0.808081   \n",
       "                                          3                        0.705882   \n",
       "                                          4                        0.734694   \n",
       "                                          5                        0.831683   \n",
       "\n",
       "                                                scientific_entities_avg_acc  \\\n",
       "model                                     fold                                \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                        0.901099   \n",
       "                                          2                        0.952381   \n",
       "                                          3                        0.904762   \n",
       "                                          4                        0.897436   \n",
       "                                          5                        0.919118   \n",
       "\n",
       "                                                ...  \\\n",
       "model                                     fold  ...   \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1     ...   \n",
       "                                          2     ...   \n",
       "                                          3     ...   \n",
       "                                          4     ...   \n",
       "                                          5     ...   \n",
       "\n",
       "                                                scientific_entities_avg_rec  \\\n",
       "model                                     fold                                \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                        0.941176   \n",
       "                                          2                        0.970588   \n",
       "                                          3                        0.808824   \n",
       "                                          4                        0.808824   \n",
       "                                          5                        0.882353   \n",
       "\n",
       "                                                scientific_entities_avg_f1  \\\n",
       "model                                     fold                               \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                       0.825806   \n",
       "                                          2                       0.910345   \n",
       "                                          3                       0.808824   \n",
       "                                          4                       0.797101   \n",
       "                                          5                       0.845070   \n",
       "\n",
       "                                                macro_acc  macro_prec  \\\n",
       "model                                     fold                          \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1      0.772894    0.738288   \n",
       "                                          2      0.820513    0.826342   \n",
       "                                          3      0.765568    0.754544   \n",
       "                                          4      0.783883    0.765873   \n",
       "                                          5      0.790441    0.783911   \n",
       "\n",
       "                                                macro_rec  macro_f1  runtime  \\\n",
       "model                                     fold                                 \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1      0.871941  0.797756  24.1422   \n",
       "                                          2      0.858715  0.841327  21.4280   \n",
       "                                          3      0.795993  0.774218  18.8865   \n",
       "                                          4      0.805904  0.784103  19.8471   \n",
       "                                          5      0.879673  0.827211  23.1913   \n",
       "\n",
       "                                                samples_per_second  \\\n",
       "model                                     fold                       \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                 11.308   \n",
       "                                          2                 12.740   \n",
       "                                          3                 14.455   \n",
       "                                          4                 13.755   \n",
       "                                          5                 11.729   \n",
       "\n",
       "                                                steps_per_second  poch  \n",
       "model                                     fold                          \n",
       "cardiffnlp/twitter-roberta-base-2022-154m 1                0.083  10.0  \n",
       "                                          2                0.093  10.0  \n",
       "                                          3                0.106  10.0  \n",
       "                                          4                0.101  10.0  \n",
       "                                          5                0.086  10.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(evals).set_index([\"model\", \"fold\"]).rename(columns=lambda x: x.lstrip(\"_\"))\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oss</th>\n",
       "      <th>scientific_claim_avg_acc</th>\n",
       "      <th>scientific_claim_avg_prec</th>\n",
       "      <th>scientific_claim_avg_rec</th>\n",
       "      <th>scientific_claim_avg_f1</th>\n",
       "      <th>scientific_reference_avg_acc</th>\n",
       "      <th>scientific_reference_avg_prec</th>\n",
       "      <th>scientific_reference_avg_rec</th>\n",
       "      <th>scientific_reference_avg_f1</th>\n",
       "      <th>scientific_entities_avg_acc</th>\n",
       "      <th>...</th>\n",
       "      <th>scientific_entities_avg_rec</th>\n",
       "      <th>scientific_entities_avg_f1</th>\n",
       "      <th>macro_acc</th>\n",
       "      <th>macro_prec</th>\n",
       "      <th>macro_rec</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>runtime</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>poch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cardiffnlp/twitter-roberta-base-2022-154m</th>\n",
       "      <td>0.329868</td>\n",
       "      <td>0.889291</td>\n",
       "      <td>0.760155</td>\n",
       "      <td>0.848983</td>\n",
       "      <td>0.800556</td>\n",
       "      <td>0.916438</td>\n",
       "      <td>0.761594</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.776782</td>\n",
       "      <td>0.914959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.837429</td>\n",
       "      <td>0.78666</td>\n",
       "      <td>0.773791</td>\n",
       "      <td>0.842445</td>\n",
       "      <td>0.804923</td>\n",
       "      <td>21.49902</td>\n",
       "      <td>12.7974</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                oss  scientific_claim_avg_acc  \\\n",
       "model                                                                           \n",
       "cardiffnlp/twitter-roberta-base-2022-154m  0.329868                  0.889291   \n",
       "\n",
       "                                           scientific_claim_avg_prec  \\\n",
       "model                                                                  \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                   0.760155   \n",
       "\n",
       "                                           scientific_claim_avg_rec  \\\n",
       "model                                                                 \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                  0.848983   \n",
       "\n",
       "                                           scientific_claim_avg_f1  \\\n",
       "model                                                                \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                 0.800556   \n",
       "\n",
       "                                           scientific_reference_avg_acc  \\\n",
       "model                                                                     \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                      0.916438   \n",
       "\n",
       "                                           scientific_reference_avg_prec  \\\n",
       "model                                                                      \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                       0.761594   \n",
       "\n",
       "                                           scientific_reference_avg_rec  \\\n",
       "model                                                                     \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                         0.796   \n",
       "\n",
       "                                           scientific_reference_avg_f1  \\\n",
       "model                                                                    \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                     0.776782   \n",
       "\n",
       "                                           scientific_entities_avg_acc  ...  \\\n",
       "model                                                                   ...   \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                     0.914959  ...   \n",
       "\n",
       "                                           scientific_entities_avg_rec  \\\n",
       "model                                                                    \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                     0.882353   \n",
       "\n",
       "                                           scientific_entities_avg_f1  \\\n",
       "model                                                                   \n",
       "cardiffnlp/twitter-roberta-base-2022-154m                    0.837429   \n",
       "\n",
       "                                           macro_acc  macro_prec  macro_rec  \\\n",
       "model                                                                         \n",
       "cardiffnlp/twitter-roberta-base-2022-154m    0.78666    0.773791   0.842445   \n",
       "\n",
       "                                           macro_f1   runtime  \\\n",
       "model                                                           \n",
       "cardiffnlp/twitter-roberta-base-2022-154m  0.804923  21.49902   \n",
       "\n",
       "                                           samples_per_second  \\\n",
       "model                                                           \n",
       "cardiffnlp/twitter-roberta-base-2022-154m             12.7974   \n",
       "\n",
       "                                           steps_per_second  poch  \n",
       "model                                                              \n",
       "cardiffnlp/twitter-roberta-base-2022-154m            0.0938  10.0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df.groupby(\"model\").mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
