{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Baselines\n",
    "\n",
    "Rewritten baseline/fine tuning example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "upstream = None\n",
    "product = None\n",
    "some_param = None\n",
    "\n",
    "model_id = \"cardiffnlp/twitter-roberta-large-2022-154m\"\n",
    "\n",
    "warmup_ratio = 0.1\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 10\n",
    "weight_decay = 0.01\n",
    "per_device_train_batch_size = 32\n",
    "per_device_eval_batch_size = 256\n",
    "gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from climatesense_checkthat2025.utils.data import (\n",
    "    compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directory exists:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    ROOT_DIR = \"../../data/\"\n",
    "else:\n",
    "    ROOT_DIR = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics_multilabel_sequenceclassification(\n",
    "    eval_predictions: EvalPrediction, threshold: float = 0.5, labels: List[str] = None\n",
    "):\n",
    "    \"\"\"Compute metrics for a multi-label sequence classification model's predictions.\n",
    "\n",
    "    This function applies a sigmoid activation function to the model's raw logits to calculate probabilities,\n",
    "    converts probabilities to binary predictions based on a specified threshold, and computes evaluation metrics\n",
    "    using the provided `compute_metrics` function.\n",
    "\n",
    "    Args:\n",
    "        eval_predictions (EvalPrediction): An object containing the model's predictions and the true labels.\n",
    "            - `eval_predictions.predictions`: The raw logits output by the model.\n",
    "            - `eval_predictions.label_ids`: The true labels for the predictions.\n",
    "        threshold (float, optional): The threshold for converting probabilities to binary predictions. Defaults to 0.5.\n",
    "        labels (List[str], optional): A list of label names for the metrics computation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed evaluation metrics.\n",
    "    \"\"\"\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    x_test = eval_predictions.predictions\n",
    "    y_test = eval_predictions.label_ids\n",
    "\n",
    "    # Calculate probabilities and derive binary predictions:\n",
    "    probs = sigmoid(torch.Tensor(x_test))\n",
    "    y_pred = torch.where(probs >= threshold, 1.0, 0.0)\n",
    "\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu()\n",
    "\n",
    "    return compute_metrics(y_pred, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_loss(outputs, labels, num_items_in_batch=None, return_outputs=False, class_weights=None):\n",
    "    logits = outputs.get(\"logits\")\n",
    "    n_labels = logits.shape[1]\n",
    "\n",
    "    loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=torch.from_numpy(class_weights).float().to(device=logits.device))\n",
    "    loss = loss_fct(logits.view(-1, n_labels), labels.view(-1, n_labels)).float().to(device=logits.device)\n",
    "\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the provided trainning and test data:\n",
    "subtask4a_train_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"processed/task4/subtask_4a/ct_train_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subtask4a_test_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"processed/task4/subtask_4a/ct_dev_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subtask4a_eval_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"processed/task4/subtask_4a/ct_eval_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev Evaluation\n",
    "\n",
    "We use the train/dev from the repository rathert than the folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 1228\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 137\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataset:\n",
    "\n",
    "ds_train = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": subtask4a_train_df[\"text\"],\n",
    "        \"labels\": subtask4a_train_df[\n",
    "            [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "        ].values.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_dev = Dataset.from_dict(\n",
    "    {\n",
    "        \"text\": subtask4a_test_df[\"text\"],\n",
    "        \"labels\": subtask4a_test_df[\n",
    "            [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "        ].values.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_eval = Dataset.from_dict({\"text\": subtask4a_eval_df[\"text\"]})\n",
    "\n",
    "ds = DatasetDict({\"train\": ds_train, \"test\": ds_dev, \"eval\": ds_eval})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = np.array(ds[\"train\"][\"labels\"]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, EarlyStoppingCallback\n",
    "\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = f\"baseline-*-{model_id.split('/')[-1]}\"\n",
    "os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\"\n",
    "\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    num_labels=3,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer=tokenizer):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    output_dir=os.path.join(ROOT_DIR, f\"results/baselines/checkpoints/{model_id.split('/')[-1]}\"),\n",
    "    logging_dir=os.path.join(ROOT_DIR, f\"results/baselines/logs/{model_id.split('/')[-1]}\"),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=partial(\n",
    "        compute_metrics_multilabel_sequenceclassification,\n",
    "        labels=[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"],\n",
    "    ),\n",
    "    # compute_loss_func=partial(compute_weighted_loss, class_weights=class_weights),\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate()\n",
    "eval_result[\"model\"] = model_id\n",
    "\n",
    "pprint(eval_result)\n",
    "eval_result  # noqa: B018\n",
    "\n",
    "model.save_pretrained(os.path.join(ROOT_DIR, f\"results/baselines/checkpoints/{model_id.split('/')[-1]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>@user Nabil these sickos full stop preying on ...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>So do strippers just wait to be saved to stop ...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>The weaponization of medical language embolden...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>#sports #fitness Buy Now: $33.97 Ueasy Knee pa...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>@user thx for support also trans community - a...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1317</td>\n",
       "      <td>Winter Tipples tasting went down a treat last ...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1327</td>\n",
       "      <td>@user Please read this research analysis https...</td>\n",
       "      <td>[0.0, 1.0, 1.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1329</td>\n",
       "      <td>@user Stop hoarding so much and throw some shi...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1343</td>\n",
       "      <td>@user ugh. I'm really over the mezzo drama. th...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1354</td>\n",
       "      <td>Ran out of Propane Halfway Through Grilling: A...</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text  \\\n",
       "0       11  @user Nabil these sickos full stop preying on ...   \n",
       "1       23  So do strippers just wait to be saved to stop ...   \n",
       "2       28  The weaponization of medical language embolden...   \n",
       "3       32  #sports #fitness Buy Now: $33.97 Ueasy Knee pa...   \n",
       "4       57  @user thx for support also trans community - a...   \n",
       "..     ...                                                ...   \n",
       "132   1317  Winter Tipples tasting went down a treat last ...   \n",
       "133   1327  @user Please read this research analysis https...   \n",
       "134   1329  @user Stop hoarding so much and throw some shi...   \n",
       "135   1343  @user ugh. I'm really over the mezzo drama. th...   \n",
       "136   1354  Ran out of Propane Halfway Through Grilling: A...   \n",
       "\n",
       "              labels  scientific_claim  scientific_reference  \\\n",
       "0    [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "1    [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "2    [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "3    [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "4    [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "..               ...               ...                   ...   \n",
       "132  [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "133  [0.0, 1.0, 1.0]               0.0                   1.0   \n",
       "134  [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "135  [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "136  [0.0, 0.0, 0.0]               0.0                   0.0   \n",
       "\n",
       "     scientific_entities  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "..                   ...  \n",
       "132                  0.0  \n",
       "133                  1.0  \n",
       "134                  0.0  \n",
       "135                  0.0  \n",
       "136                  0.0  \n",
       "\n",
       "[137 rows x 6 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtask4a_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using the model:\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=os.path.join(\n",
    "        ROOT_DIR,\n",
    "        \"results/baselines/checkpoints/twitter-roberta-large-2022-154m-unbalanced/checkpoint-300\",\n",
    "    ),\n",
    "    num_labels=3,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label={\n",
    "        0: \"scientific_claim\",\n",
    "        1: \"scientific_reference\",\n",
    "        2: \"scientific_entities\",\n",
    "    },\n",
    "    label2id={\n",
    "        \"scientific_claim\": 0,\n",
    "        \"scientific_reference\": 1,\n",
    "        \"scientific_entities\": 2,\n",
    "    },\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# # Create a text classification pipeline\n",
    "clf = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_hidden_states(text, model, tokenizer):\n",
    "    \"\"\"Generate the last hidden states for a given text using a pre-trained model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to process.\n",
    "        model: The pre-trained model to use for generating hidden states.\n",
    "        tokenizer: The tokenizer corresponding to the pre-trained model.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The mean of the last hidden states for the input text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_states = outputs.hidden_states[-1].mean(dim=1).to(\"cpu\").detach().numpy()\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = subtask4a_train_df[\"text\"].apply(lambda x: get_last_hidden_states(x, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best for step each label:\n",
    "best_steps = {\n",
    "    \"scientific_claim\": 210,\n",
    "    \"scientific_reference\": 240,\n",
    "    \"scientific_entities\": 105,\n",
    "}\n",
    "\n",
    "embeddings = {}\n",
    "for _, (k, v) in enumerate(best_steps.items()):\n",
    "    print(k, v)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=os.path.join(\n",
    "            ROOT_DIR,\n",
    "            f\"results/baselines/checkpoints/twitter-roberta-large-2022-154m-unbalanced/checkpoint-{v}\",\n",
    "        ),\n",
    "        num_labels=3,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        id2label={\n",
    "            0: \"scientific_claim\",\n",
    "            1: \"scientific_reference\",\n",
    "            2: \"scientific_entities\",\n",
    "        },\n",
    "        label2id={\n",
    "            \"scientific_claim\": 0,\n",
    "            \"scientific_reference\": 1,\n",
    "            \"scientific_entities\": 2,\n",
    "        },\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    emb = subtask4a_train_df[\"text\"].apply(\n",
    "        lambda x: get_last_hidden_states(x, model, tokenizer)  # noqa: B023\n",
    "    )\n",
    "\n",
    "    embeddings[k] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best for step each label:\n",
    "best_steps = {\n",
    "    \"scientific_claim\": 210,\n",
    "    \"scientific_reference\": 240,\n",
    "    \"scientific_entities\": 105,\n",
    "}\n",
    "\n",
    "test_embeddings = {}\n",
    "for _, (k, v) in enumerate(best_steps.items()):\n",
    "    print(k, v)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=os.path.join(\n",
    "            ROOT_DIR,\n",
    "            f\"results/baselines/checkpoints/twitter-roberta-large-2022-154m-unbalanced/checkpoint-{v}\",\n",
    "        ),\n",
    "        num_labels=3,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        id2label={\n",
    "            0: \"scientific_claim\",\n",
    "            1: \"scientific_reference\",\n",
    "            2: \"scientific_entities\",\n",
    "        },\n",
    "        label2id={\n",
    "            \"scientific_claim\": 0,\n",
    "            \"scientific_reference\": 1,\n",
    "            \"scientific_entities\": 2,\n",
    "        },\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    emb = subtask4a_test_df[\"text\"].apply(\n",
    "        lambda x: get_last_hidden_states(x, model, tokenizer)  # noqa: B023\n",
    "    )\n",
    "\n",
    "    test_embeddings[k] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best for step each label:\n",
    "best_steps = {\n",
    "    \"scientific_claim\": 210,\n",
    "    \"scientific_reference\": 240,\n",
    "    \"scientific_entities\": 105,\n",
    "}\n",
    "\n",
    "eval_embeddings = {}\n",
    "for _, (k, v) in enumerate(best_steps.items()):\n",
    "    print(k, v)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=os.path.join(\n",
    "            ROOT_DIR,\n",
    "            f\"results/baselines/checkpoints/twitter-roberta-large-2022-154m-unbalanced/checkpoint-{v}\",\n",
    "        ),\n",
    "        num_labels=3,\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        id2label={\n",
    "            0: \"scientific_claim\",\n",
    "            1: \"scientific_reference\",\n",
    "            2: \"scientific_entities\",\n",
    "        },\n",
    "        label2id={\n",
    "            \"scientific_claim\": 0,\n",
    "            \"scientific_reference\": 1,\n",
    "            \"scientific_entities\": 2,\n",
    "        },\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    emb = subtask4a_eval_df[\"text\"].apply(\n",
    "        lambda x: get_last_hidden_states(x, model, tokenizer)  # noqa: B023\n",
    "    )\n",
    "\n",
    "    eval_embeddings[k] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read heuristic:\n",
    "features = [\n",
    "    \"is_claim_with_sciterm\",\n",
    "    \"is_claim\",\n",
    "    \"contains_arg\",\n",
    "    \"contains_scientific_term\",\n",
    "    \"has_url\",\n",
    "    \"has_sci_domain\",\n",
    "    \"has_sci_subdomain\",\n",
    "    \"has_sci_mag_domain\",\n",
    "    \"has_sci_news_domain\",\n",
    "    \"is_related_to_research\",\n",
    "    \"mentions_science_research_in_general\",\n",
    "    \"mentions_scientist\",\n",
    "    \"mentions_publications\",\n",
    "    \"mentions_research_method\",\n",
    "]\n",
    "\n",
    "train_heuristic_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"processed/task4/subtask_4a/ct_train_clean_heuristics.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "test_heuristic_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"processed/task4/subtask_4a/ct_dev_clean_heuristics.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "eval_heuristic_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"processed/task4/subtask_4a/ct_eval_heuristics.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "for k in embeddings.keys():\n",
    "    print(k)\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=f1_score)\n",
    "\n",
    "    # Reshape X_train and X_test to 2D\n",
    "    X_train = np.array(embeddings[k].tolist()).reshape(len(embeddings[k]), -1)\n",
    "    # X_train = np.hstack((X_train, train_heuristic_df[features].replace({True: 1, False: 0}).values)) # Add heuristics\n",
    "    y_train = subtask4a_train_df[k].values.tolist()\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    X_test = np.array(test_embeddings[k].tolist()).reshape(len(test_embeddings[k]), -1)\n",
    "    # X_test = np.hstack((X_test, test_heuristic_df[features].replace({True: 1, False: 0}).values)) # Add heuristics\n",
    "    y_test = subtask4a_test_df[k].values.tolist()\n",
    "\n",
    "    models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "    display(models.sort_values(\"f1_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "best_clf = {\n",
    "    \"scientific_claim\": NearestCentroid(),\n",
    "    \"scientific_reference\": GaussianNB(),\n",
    "    \"scientific_entities\": NearestCentroid(),\n",
    "}\n",
    "preds_clf = {}\n",
    "eval_preds_clf = {}\n",
    "for k, clf in best_clf.items():\n",
    "    print(k, clf)\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "    # Reshape X_train and X_test to 2D\n",
    "    X_train = np.array(embeddings[k].tolist()).reshape(len(embeddings[k]), -1)\n",
    "    # X_train = np.hstack((X_train, train_heuristic_df[features].replace({True: 1, False: 0}).values)) # Add heuristics\n",
    "    y_train = subtask4a_train_df[k].values.tolist()\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    X_test = np.array(test_embeddings[k].tolist()).reshape(len(test_embeddings[k]), -1)\n",
    "    # X_test = np.hstack((X_test, test_heuristic_df[features].replace({True: 1, False: 0}).values)) # Add heuristics\n",
    "    y_test = subtask4a_test_df[k].values.tolist()\n",
    "\n",
    "    X_eval = np.array(eval_embeddings[k].tolist()).reshape(len(eval_embeddings[k]), -1)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds_clf[k] = clf.predict(X_test)\n",
    "    eval_preds_clf[k] = clf.predict(X_eval)\n",
    "\n",
    "    print(f1_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    np.array(preds_clf[\"scientific_claim\"]),\n",
    "    preds_clf[\"scientific_reference\"],\n",
    "    np.array(preds_clf[\"scientific_entities\"]).tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(\n",
    "    pd.DataFrame(preds_clf)[[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]].values.tolist(),\n",
    "    subtask4a_test_df[[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]].values,\n",
    "    labels=[\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submission_df = pd.DataFrame(preds_clf)\n",
    "test_submission_df = test_submission_df.rename(\n",
    "    columns={\n",
    "        \"scientific_claim\": \"cat1_pred\",\n",
    "        \"scientific_reference\": \"cat2_pred\",\n",
    "        \"scientific_entities\": \"cat3_pred\",\n",
    "    }\n",
    ")\n",
    "test_submission_df[\"index\"] = subtask4a_test_df[\"index\"]\n",
    "test_submission_df = test_submission_df[[\"index\", \"cat1_pred\", \"cat2_pred\", \"cat3_pred\"]]\n",
    "\n",
    "\n",
    "# Save the submission file and zip it:\n",
    "\n",
    "test_submission_df.to_csv(\n",
    "    os.path.join(ROOT_DIR, \"results/baselines/predictions.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "# Zip the submission file:\n",
    "shutil.make_archive(\n",
    "    os.path.join(ROOT_DIR, \"results/baselines/test_predictions\"),\n",
    "    \"zip\",\n",
    "    os.path.join(ROOT_DIR, \"results/baselines/\"),\n",
    "    \"predictions.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### eval ########################\n",
    "eval_submission_df = pd.DataFrame(eval_preds_clf)\n",
    "eval_submission_df = eval_submission_df.rename(\n",
    "    columns={\n",
    "        \"scientific_claim\": \"cat1_pred\",\n",
    "        \"scientific_reference\": \"cat2_pred\",\n",
    "        \"scientific_entities\": \"cat3_pred\",\n",
    "    }\n",
    ")\n",
    "eval_submission_df[\"index\"] = subtask4a_eval_df[\"index\"]\n",
    "eval_submission_df = eval_submission_df[[\"index\", \"cat1_pred\", \"cat2_pred\", \"cat3_pred\"]]\n",
    "\n",
    "\n",
    "# Save the submission file and zip it:\n",
    "\n",
    "eval_submission_df.to_csv(\n",
    "    os.path.join(ROOT_DIR, \"results/baselines/predictions.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "# Zip the submission file:\n",
    "shutil.make_archive(\n",
    "    os.path.join(ROOT_DIR, \"results/baselines/predictions\"),\n",
    "    \"zip\",\n",
    "    os.path.join(ROOT_DIR, \"results/baselines/\"),\n",
    "    \"predictions.csv\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat2025-task4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
