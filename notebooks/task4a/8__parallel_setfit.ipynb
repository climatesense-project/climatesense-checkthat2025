{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532ba038",
   "metadata": {},
   "source": [
    "# Setfit but using separate predictors and the augmented data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c9bd3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# parameters\n",
    "upstream = None\n",
    "product = None\n",
    "some_param = None\n",
    "\n",
    "\n",
    "model_name = \"bge-base-en-v1.5\"\n",
    "model_id = \"BAAI/bge-base-en-v1.5\"\n",
    "epochs = 1\n",
    "num_iterations = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import setfit\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directory exists:\n",
    "if not os.path.exists(\"./data\"):\n",
    "    ROOT_DIR = \"../../data/processed/task4/subtask_4a/\"\n",
    "else:\n",
    "    ROOT_DIR = \"./data/processed/task4/subtask_4a/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735a4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1974814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "\n",
    "def compute_metrics_sequenceclassification(eval_predictions: EvalPrediction):\n",
    "    \"\"\"Compute metrics for a sequence classification model's predictions.\n",
    "\n",
    "    This function applies a softmax activation function to the model's raw logits to calculate probabilities,\n",
    "    converts probabilities to binary predictions based on a specified threshold, and computes evaluation metrics\n",
    "    using the provided `compute_metrics` function.\n",
    "\n",
    "    Args:\n",
    "        eval_predictions (EvalPrediction): An object containing the model's predictions and the true labels.\n",
    "            - `eval_predictions.predictions`: The raw logits output by the model.\n",
    "            - `eval_predictions.label_ids`: The true labels for the predictions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed evaluation metrics.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_predictions\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    print(f\"f1: {f1.compute(predictions=predictions, references=labels)}\")\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels),\n",
    "        \"precision\": precision.compute(predictions=predictions, references=labels),\n",
    "        \"recall\": recall.compute(predictions=predictions, references=labels),\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21da080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def compute_binary_metrics(y_pred, y_test):\n",
    "    y_pred = np.array(y_pred)  # , copy=None)\n",
    "    y_test = np.array(y_test)  # , copy=None)\n",
    "    metrics = {}\n",
    "\n",
    "    metrics[\"acc\"] = accuracy_score(y_test, y_pred)\n",
    "    metrics[\"prec\"] = precision_score(y_test, y_pred)\n",
    "    metrics[\"rec\"] = recall_score(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "    )\n",
    "    metrics[\"f1\"] = f1_score(y_test, y_pred)\n",
    "\n",
    "    metrics[\"macro_prec\"] = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    metrics[\"macro_rec\"] = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    metrics[\"macro_f1\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2656d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_pred, y_test, labels: List[str] = None):\n",
    "    y_pred = np.array(y_pred)  # , copy=None)\n",
    "    y_test = np.array(y_test)  # , copy=None)\n",
    "    metrics = {}\n",
    "\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for multi-label classification.\n",
    "\n",
    "    Args:\n",
    "        y_pred (np.ndarray): Predicted labels, shape (n_samples, n_labels).\n",
    "        y_test (np.ndarray): True labels, shape (n_samples, n_labels).\n",
    "        labels (List[str], optional): List of label names. If None, numeric indices are used.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing accuracy, precision, recall, and F1 score for each label,\n",
    "        as well as the macro accuracy, precision, recall, and F1 score across all labels.\n",
    "    \"\"\"\n",
    "    if (labels is None) or (len(labels) != y_test.shape[1]):\n",
    "        labels = list(range(0, y_test.shape[1]))\n",
    "\n",
    "    for i in range(0, y_test.shape[1]):\n",
    "        acc = accuracy_score(y_test[:, i], y_pred[:, i])\n",
    "        prec = precision_score(y_test[:, i], y_pred[:, i], zero_division=0)\n",
    "        rec = recall_score(y_test[:, i], y_pred[:, i], zero_division=0)\n",
    "        f1 = f1_score(y_test[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "        metrics.update(\n",
    "            {\n",
    "                f\"{labels[i]}_avg_acc\": acc,\n",
    "                f\"{labels[i]}_avg_prec\": prec,\n",
    "                f\"{labels[i]}_avg_rec\": rec,\n",
    "                f\"{labels[i]}_avg_f1\": f1,\n",
    "            }\n",
    "        )\n",
    "    metrics[\"macro_acc\"] = accuracy_score(y_test, y_pred)\n",
    "    metrics[\"macro_prec\"] = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    metrics[\"macro_rec\"] = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    metrics[\"macro_f1\"] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"scientific_claim\", \"scientific_reference\", \"scientific_entities\"]\n",
    "# columns = [\"scientific_reference\", \"scientific_entities\"]\n",
    "\n",
    "\n",
    "# Load the provided trainning and test data:\n",
    "subtask4a_train_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"ct_train_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "subtask4a_test_df = pd.read_csv(\n",
    "    os.path.join(ROOT_DIR, \"ct_dev_clean.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00185a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from setfit import SetFitModel\n",
    "from transformers import (\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "\n",
    "def fine_tune_setfit_binary_classifier(\n",
    "    model_name: str,\n",
    "    train_texts: List[str],\n",
    "    train_labels: List[int],\n",
    "    val_texts: List[str],\n",
    "    val_labels: List[int],\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 16,\n",
    "    num_iterations: int = 1,\n",
    ") -> Trainer:\n",
    "    \"\"\"Fine-tune a binary classifier with SetFit.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Pretrained model name (e.g., 'bert-base-uncased').\n",
    "        train_texts (List[str]): List of training texts.\n",
    "        train_labels (List[int]): List of training labels (0 or 1).\n",
    "        val_texts (List[str]): List of validation texts.\n",
    "        val_labels (List[int]): List of validation labels (0 or 1).\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "        num_iterations (int): Number of contrastive learning iterations per epoch.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: Hugging Face Trainer object after training.\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    model = SetFitModel.from_pretrained(model_name)\n",
    "\n",
    "    # Create Dataset:\n",
    "    ds_train = Dataset.from_dict({\"text\": train_texts, \"labels\": train_labels})\n",
    "    ds_dev = Dataset.from_dict({\"text\": val_texts, \"labels\": val_labels})\n",
    "    ds = DatasetDict({\"train\": ds_train, \"test\": ds_dev})\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = setfit.TrainingArguments(\n",
    "        num_epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        # learning_rate=2e-5, #learning_rate=1e-6,\n",
    "        # weight_decay=0.01,\n",
    "        # fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        # bf16 = torch.cuda.is_bf16_supported(),\n",
    "        # metric_for_best_model=\"f1\",\n",
    "        # greater_is_better=True,\n",
    "        num_iterations=num_iterations,\n",
    "    )\n",
    "\n",
    "    # Define Trainer\n",
    "    trainer = setfit.Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"test\"],\n",
    "        metric=compute_binary_metrics,\n",
    "        column_mapping={\"text\": \"text\", \"labels\": \"label\"},\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e003020",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f556edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = {}\n",
    "predictions = {}\n",
    "for cl in columns:\n",
    "    # Load the data\n",
    "    # Check if oversampling file exists:\n",
    "    if os.path.exists(os.path.join(ROOT_DIR, f\"ct_train_oversamples_{cl}.tsv\")):\n",
    "        subtask4a_cat_claim_train_df = pd.read_csv(\n",
    "            os.path.join(ROOT_DIR, f\"ct_train_oversamples_{cl}.tsv\"),\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"text\", cl],\n",
    "        )\n",
    "\n",
    "        print(f\"Evaluating {cl}...\")\n",
    "        for dataset_df, name in [\n",
    "            (subtask4a_cat_claim_train_df, \"oversampling\"),\n",
    "            (subtask4a_train_df[[\"text\", cl]], \"training\"),\n",
    "            (subtask4a_test_df[[\"text\", cl]], \"evaluation\"),\n",
    "        ]:\n",
    "            if name == \"oversampling\":\n",
    "                oversampling_train_df = dataset_df\n",
    "            elif name == \"training\":\n",
    "                train_df = dataset_df = dataset_df\n",
    "            elif name == \"evaluation\":\n",
    "                eval_df = dataset_df = dataset_df\n",
    "\n",
    "        # Train model with oversampling + training:\n",
    "        X_train = np.array(pd.concat([train_df, oversampling_train_df])[\"text\"].tolist())\n",
    "        y_train = list(map(int, pd.concat([train_df, oversampling_train_df])[cl]))\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "        X_test = np.array(subtask4a_test_df[\"text\"].tolist())\n",
    "        y_test = list(map(int, subtask4a_test_df[cl]))\n",
    "\n",
    "        model, trainer = fine_tune_setfit_binary_classifier(\n",
    "            model_name=model_id,\n",
    "            train_texts=X_train,\n",
    "            train_labels=y_train,\n",
    "            val_texts=X_test,\n",
    "            val_labels=y_test,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            num_iterations=num_iterations,\n",
    "        )\n",
    "\n",
    "        # Evaluate the model:\n",
    "        eval_result = trainer.evaluate()\n",
    "        eval_result[\"model\"] = model_id\n",
    "\n",
    "        # Store predicitons:\n",
    "        predictions[cl] = model.predict(X_test)\n",
    "        predictions[f\"{cl}_embeddings\"] = model.encode(X_test)\n",
    "        train_embeddings[f\"{cl}_embeddings\"] = model.encode(X_train)\n",
    "\n",
    "        # Save the model:\n",
    "        model.save_pretrained(os.path.join(ROOT_DIR, f\"results/setfit_{cl}\"))\n",
    "\n",
    "        pprint(eval_result)\n",
    "\n",
    "        # Clear memory:\n",
    "        model.to(\"cpu\")\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4effc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results\n",
    "predictions_df = pd.DataFrame(\n",
    "    {\n",
    "        \"scientific_claim\": predictions[\"scientific_claim\"],\n",
    "        \"scientific_reference\": predictions[\"scientific_reference\"],\n",
    "        \"scientific_entities\": predictions[\"scientific_entities\"],\n",
    "    }\n",
    ")\n",
    "compute_metrics(\n",
    "    predictions_df[columns].values.tolist(),\n",
    "    subtask4a_test_df[columns].values.tolist(),\n",
    "    labels=columns,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatesense-checkthat2025-task4-F-tagyMC-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
